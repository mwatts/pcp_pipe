Metadata-Version: 2.4
Name: pcp-pipe
Version: 0.1.0
Summary: High-Performance Podcast Processing Pipeline with GPU acceleration and local ML models
Requires-Python: <3.13,>=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: torchaudio>=2.0.0
Requires-Dist: openai-whisper>=20230918
Requires-Dist: transformers[torch]>=4.30.0
Requires-Dist: resemblyzer>=0.1.3
Requires-Dist: scikit-learn>=1.3.0
Requires-Dist: spacy>=3.7.0
Requires-Dist: librosa>=0.10.0
Requires-Dist: soundfile>=0.12.0
Requires-Dist: yt-dlp>=2023.7.6
Requires-Dist: requests>=2.31.0
Requires-Dist: numpy<2.0,>=1.24.0
Requires-Dist: scipy>=1.11.0
Provides-Extra: dev
Requires-Dist: pytest>=8.4.1; extra == "dev"
Requires-Dist: black>=25.1.0; extra == "dev"
Requires-Dist: ruff>=0.12.7; extra == "dev"

# PCP Pipe - High-Performance Podcast Processing Pipeline

A blazing-fast, GPU-accelerated podcast processing pipeline that downloads, transcribes, analyzes, and summarizes podcast audio using state-of-the-art ML models - all running locally without external API dependencies.

## Features

- üéôÔ∏è **Advanced Transcription**: OpenAI Whisper large-v3 with word-level timestamps
- üë• **Speaker Diarization**: Automatic speaker identification using Resemblyzer
- üè∑Ô∏è **Entity Extraction**: Named entity recognition with spaCy
- üìù **Smart Summarization**: Content summaries using DistilBART
- üöÄ **Multi-Platform GPU Acceleration**: CUDA, Metal (Apple Silicon), ROCm support
- ‚ö° **Optional Mojo Acceleration**: For performance-critical operations
- üîí **Privacy-First**: All processing happens locally - no external APIs
- üåê **Universal Download**: Supports YouTube, direct URLs, and more via yt-dlp

## Quick Start

```bash
# Automated setup (recommended)
./setup_scripts.sh

# Process a podcast
./podcast_processor.sh 'https://youtube.com/watch?v=example'
```

## Installation

### System Requirements

- Python 3.8 or higher
- 8GB+ RAM recommended
- Optional: GPU with CUDA, Metal, or ROCm support

### Automated Setup (Recommended)

The setup script automatically detects your platform and installs optimized dependencies:

```bash
./setup_scripts.sh
```

This will:
- Detect GPU capabilities (CUDA, Metal, ROCm)
- Install uv package manager for fast dependency management
- Optionally install Mojo for performance acceleration
- Configure platform-specific optimizations
- Create launcher scripts

### Manual Setup

```bash
# Install dependencies
pip install torch torchaudio openai-whisper transformers[torch] resemblyzer \
            scikit-learn spacy librosa soundfile yt-dlp requests numpy scipy

# Download spaCy model
python -m spacy download en_core_web_lg
```

## Usage

### Basic Usage

```bash
# Process with automatic GPU detection
./podcast_processor.sh 'https://example.com/podcast.mp3'

# Force GPU usage
./podcast_processor.sh 'url' --use-gpu

# Use Mojo acceleration (if installed)
./podcast_processor.sh 'url' --use-mojo
```

### Direct Python Usage

```bash
python podcast_processor.py 'url' \
    --output-dir ./my_output \
    --whisper-model large-v3 \
    --device cuda  # or mps for Apple Silicon
```

### Command Line Options

- `--output-dir`: Specify output directory (default: `./output_{timestamp}`)
- `--whisper-model`: Whisper model size (default: `large-v3`)
- `--device`: Force specific device (`cuda`, `mps`, `cpu`)
- `--use-gpu`: Enable GPU acceleration
- `--use-mojo`: Enable Mojo acceleration

## Output Format

The processor creates a timestamped output directory with comprehensive results:

```
output_20240101_120000/
   {hash}_complete.json      # All processing results
   {hash}_transcript.json    # Full transcript with timestamps
   {hash}_summary.json       # Generated summary
   {hash}_entities.json      # Extracted entities
```

### Output Structure

Each JSON file contains:
- **Transcript**: Word-level timestamps, speaker labels, confidence scores
- **Summary**: Key points, processing metadata
- **Entities**: Named entities with context and locations
- **Metadata**: Processing time, optimizations used, source information

## Platform Support

| Platform | GPU Support | Acceleration |
|----------|-------------|--------------|
| macOS ARM (M1/M2/M3) | Metal (MPS) | ‚úÖ Native |
| macOS Intel | CPU only | ‚ö° Mojo via Docker |
| Linux | CUDA, ROCm | ‚úÖ Native |
| Windows | CUDA | ‚úÖ Native |

## Performance

- **Processing Speed**: Typically 5-10x faster than real-time on GPU
- **Memory Efficient**: Automatic fallback to CPU when GPU memory is insufficient
- **Async Operations**: Concurrent downloading and processing
- **Batch Processing**: Optimized NLP operations

## Models Used

All models run locally:
- **Whisper large-v3**: State-of-the-art speech recognition
- **Resemblyzer**: Voice embeddings for speaker diarization
- **spaCy en_core_web_lg**: Named entity recognition
- **DistilBART CNN**: Efficient summarization

## Development

```bash
# Install development dependencies
uv sync

# Run directly
python podcast_processor.py 'url'

# Check environment setup
./podcast_processor.sh --setup-env
```

## Troubleshooting

### GPU Not Detected
- Ensure you have the correct PyTorch version for your GPU
- Run `./setup_scripts.sh` to auto-configure
- Check GPU availability: `python -c "import torch; print(torch.cuda.is_available())"`

### Out of Memory Errors
- The processor automatically falls back to CPU
- Use a smaller Whisper model: `--whisper-model base`
- Process shorter audio segments

### Slow Processing
- Enable GPU: `--use-gpu`
- Install Mojo: Run setup script with Mojo option
- Check available resources

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues.

## License

This project is open source. See LICENSE file for details.

## Acknowledgments

Built with:
- OpenAI Whisper
- Hugging Face Transformers
- PyTorch
- spaCy
- yt-dlp
